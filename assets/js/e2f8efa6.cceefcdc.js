"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6534],{6163:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"dynamics_sidecars_storage","metadata":{"permalink":"/casskop/blog/dynamics_sidecars_storage","editUrl":"https://github.com/cscetbon/casskop/edit/master/website/blog/blog/2020-03-26-dynamics_sidecars_storage.md","source":"@site/blog/2020-03-26-dynamics_sidecars_storage.md","title":"Casskop 0.5.1 - Dynamic sidecars and storage configuration feature","description":"In a previous post, I was talking about how Setting up Cassandra Multi-Site on Google Kubernetes Engine with Casskop.","date":"2020-03-26T00:00:00.000Z","formattedDate":"March 26, 2020","tags":[{"label":"casskop","permalink":"/casskop/blog/tags/casskop"},{"label":"cassandra","permalink":"/casskop/blog/tags/cassandra"},{"label":"0.5.2","permalink":"/casskop/blog/tags/0-5-2"},{"label":"sidecars","permalink":"/casskop/blog/tags/sidecars"},{"label":"storage","permalink":"/casskop/blog/tags/storage"}],"readingTime":3.99,"truncated":false,"authors":[{"name":"Alexandre Guitton","title":"Alexandre Guitton","url":"https://github.com/erdrix","imageURL":"https://avatars0.githubusercontent.com/u/10503351?s=460&u=ea08d802388c79c17655c314296be58814391572&v=4"}],"frontMatter":{"slug":"dynamics_sidecars_storage","title":"Casskop 0.5.1 - Dynamic sidecars and storage configuration feature","author":"Alexandre Guitton","author_title":"Alexandre Guitton","author_url":"https://github.com/erdrix","author_image_url":"https://avatars0.githubusercontent.com/u/10503351?s=460&u=ea08d802388c79c17655c314296be58814391572&v=4","tags":["casskop","cassandra","0.5.2","sidecars","storage"]},"nextItem":{"title":"Multi-Casskop on Google Kubernetes Engine","permalink":"/casskop/blog/2020/01/15/multicasskop_gke"}},"content":"In a previous post, I was talking about how [Setting up Cassandra Multi-Site on Google Kubernetes Engine with Casskop](/casskop/blog/2020/01/15/multicasskop_gke).\\nSince then, two new versions [0.5.1](https://github.com/cscetbon/casskop/releases/tag/v0.5.1-release) and [0.5.2](https://github.com/cscetbon/casskop/releases/tag/v0.5.2-release) had been released.\\nIn another post, Cyril Scetbon focused on the [New Probes feature](https://medium.com/@cscetbon/new-probes-in-casskop-0-5-1-bfd1d6547967) which was added with the [PR #184\xd8(https://github.com/cscetbon/casskop/pull/184), in this post I will focus on the dynamic sidecars and storage configurations added to the operator, which give more flexibility to users to configure their Cassandra cluster deployments.\\n\\n## Purposes\\n\\nDuring our production migration from bare metal Cassandra Cluster to Kubernetes, the main challenge was to perform the smoothest transition for our OPS teams, allowing them to reuse their homemade tools, to facilitate the cluster operationalization. \\nHowever, the operator in this previous form did not leave much room for tuning statefulset and therefore the Cassandra Cluster deployed. \\nYou could use the bootstrap image to customize your cassandra node configuration, but not for the tools revolving around. \\nThat is why we added to the **CassandraCluster** the possibility to define containers into the pod in addition to the cassandra ones, these are the **sidecars**, and to configure extract storage for the pods (ie **VolumeClaimTemplates** to the Statefulset configuration).\\n\\n## Dynamics sidecars configurations\\n\\nTo keep the [container\u2019s best practices](https://cloud.google.com/blog/products/gcp/7-best-practices-for-building-containers) and address our OPS needs, we added the ability to define a dynamic list of containers into a **CassandraCluster.Spec** resource definition: [cassandracluster_types.go#L803](https://github.com/cscetbon/casskop/blob/master/pkg/apis/db/v2/cassandracluster_types.go#L803).\\n\\n```yaml \\nspec:\\n  ...\\n  sidecarConfigs:\\n    - args: [\\"tail\\", \\"-F\\", \\"/var/log/cassandra/system.log\\"]\\n      image: ez123/alpine-tini\\n      imagePullPolicy: Always\\n      name: cassandra-log\\n      resources:\\n        limits:\\n          cpu: 50m\\n          memory: 50Mi\\n        requests:\\n          cpu: 10m\\n          memory: 10Mi\\n      volumeMounts:\\n        - mountPath: /var/log/cassandra\\n          name: cassandra-logs\\n    - args: [\\"tail\\", \\"-F\\", \\"/var/log/cassandra/gc.log.0.current\\"]\\n      image: ez123/alpine-tini\\n      imagePullPolicy: Always\\n      name: gc-log\\n      resources:\\n        limits:\\n          cpu: 50m\\n          memory: 50Mi\\n        requests:\\n          cpu: 10m\\n          memory: 10Mi\\n      volumeMounts:\\n        - mountPath: /var/log/cassandra\\n          name: gc-logs\\n  ...\\n```\\n\\nThese sidecars are classic [kubernetes container resources](https://godoc.org/k8s.io/api/core/v1#Container), leaving you the full power on what you want to do. \\nWith this example, we add two simple sidecars allowing to distinguish cassandra and GC logs in two different stdout.\\n\\n:::note\\nwith this feature you can do everything you want, and obviously some bad things. This feature is not here to make a Cassandra Cluster works, the operator has everything for this, but to allow you to simplify some add-ons usage around Cassandra.\\n:::\\n\\n## Sidecars : environment variables\\n\\nAll sidecars added with this configuration will have, at the container init, some of the environment variables from **cassandra container** merged with those defined into the sidecar container\\n\\n- CASSANDRA_CLUSTER_NAME\\n- CASSANDRA_SEEDS\\n- CASSANDRA_DC\\n- CASSANDRA_RACK\\n\\n## Storage configuration\\n\\nIn the previous version, the only option about storage was the [data volume configuration](/casskop/docs/configuration_deployment/storage) allowing you to define :\\n\\n- `dataCapacity`: Defines the size of the persistent volume claim, for example, \\"1000Gi\\".\\n- `dataStorageClass`: Defines the type of storage to use (or use default one). We recommend to use local-storage for better performances but it can be any storage with high ssd throughput.\\n\\nThe dynamic sidecar doesn\u2019t really suit, unless you put everything in one folder.\\n\\n:::warning Spoiler alert\\nIt\u2019s not a good idea\\n:::\\n\\nThat is why we add the `CassandraCluster.Spec.StorageConfig` field, to the `CassandraCluster` resource definition :\\n\\n```yaml\\nspec:\\n ...\\n storageConfigs:\\n   - mountPath: \\"/var/lib/cassandra/log\\"\\n     name: \\"gc-logs\\"\\n     pvcSpec:\\n       accessModes:\\n         - ReadWriteOnce\\n       storageClassName: local-storage\\n       resources:\\n         requests:\\n           storage: 5Gi\\n   - mountPath: \\"/var/log/cassandra\\"\\n     name: \\"cassandra-logs\\"\\n     pvcSpec:\\n       accessModes:\\n         - ReadWriteOnce\\n       storageClassName: local-storage\\n         resources:\\n           requests:\\n             storage: 10Gi\\n ...\\n```\\n\\n`storageConfigs` : Defines the list of storage config object, which will instantiate `Persitence Volume Claim` and associate volume to pod of cassandra node.\\n\\n- `mountPath`: Defines the path into cassandra container where the volume will be mounted.\\n- `name`: Used to define the PVC and VolumeMount names.\\n- `pvcSpec`: pvcSpec describes the PVC used for the mountPath described above, it requires a kubernetes PVC spec.\\n\\nIn this example, we add the two volumes required by our sidecars previously configured, to be able via the sidecars to access to the logs that we want to expose on the stdout.\\n\\n## Volume Claim Template and statefulset\\n\\nKeep in mind that Casskop operator works on Statefulset, but have some constraints such as :\\n\\n```log\\nupdates to statefulset spec for fields other than \'replicas\', \'template\', and \'updateStrategy\' are forbidden.\\n```\\n\\nSo if you want to add or remove some storages configurations, today you have to perform manually it, by removing the Statefulset, which will be recreated by the operator.\\n\\n:::note\\nIt\u2019s not a sake operation, and should be performed carefully, because you will loose a rack. Maybe in some releases we will manage it, but today we assume that this operation is an exceptional one.\\n:::\\n\\n[CassKop](https://github.com/cscetbon/casskop) is open source so don\u2019t hesitate to try it out, contribute by first trying to fix a discovered issue and let\u2019s enhance it together!\\n\\nIn a next post, I will speak about the IP management into Casskop, and the [cross IPs issue](https://github.com/cscetbon/casskop/issues/170), so stay connected !"},{"id":"/2020/01/15/multicasskop_gke","metadata":{"permalink":"/casskop/blog/2020/01/15/multicasskop_gke","editUrl":"https://github.com/cscetbon/casskop/edit/master/website/blog/blog/2020-01-15-multicasskop_gke.md","source":"@site/blog/2020-01-15-multicasskop_gke.md","title":"Multi-Casskop on Google Kubernetes Engine","description":"Pre-requisites","date":"2020-01-15T00:00:00.000Z","formattedDate":"January 15, 2020","tags":[{"label":"gke","permalink":"/casskop/blog/tags/gke"},{"label":"casskop","permalink":"/casskop/blog/tags/casskop"},{"label":"cassandra","permalink":"/casskop/blog/tags/cassandra"},{"label":"external-dns","permalink":"/casskop/blog/tags/external-dns"},{"label":"multi-casskop","permalink":"/casskop/blog/tags/multi-casskop"}],"readingTime":5.64,"truncated":false,"authors":[{"name":"Alexandre Guitton","title":"Alexandre Guitton","url":"https://github.com/erdrix","imageURL":"https://avatars0.githubusercontent.com/u/10503351?s=460&u=ea08d802388c79c17655c314296be58814391572&v=4"}],"frontMatter":{"title":"Multi-Casskop on Google Kubernetes Engine","author":"Alexandre Guitton","author_title":"Alexandre Guitton","author_url":"https://github.com/erdrix","author_image_url":"https://avatars0.githubusercontent.com/u/10503351?s=460&u=ea08d802388c79c17655c314296be58814391572&v=4","tags":["gke","casskop","cassandra","external-dns","multi-casskop"]},"prevItem":{"title":"Casskop 0.5.1 - Dynamic sidecars and storage configuration feature","permalink":"/casskop/blog/dynamics_sidecars_storage"}},"content":"## Pre-requisites\\n\\nUser should need :\\n\\n* [terraform](https://learn.hashicorp.com/terraform/getting-started/install.html) version v0.12.7+\\n* [kubectl](https://kubernetes.io/fr/docs/tasks/tools/install-kubectl) version v1.13.3+\\n* [kubectx](https://github.com/ahmetb/kubectx) & kubens\\n* [Helm](https://helm.sh/docs/intro/using_helm/) version v2.15.1+\\n* [gcloud sdk](https://cloud.google.com/sdk/install?hl=fr) version 272.0.0+\\n* A service account with enough rights (for this example : `editor`)\\n* Having a DNS zone in google cloud dns.\\n\\n## Setup GCP environment\\n\\nTo setup the GCP environment we will use terraform provisionning, to instantiate the following infrastructure :\\n\\n* 2 GKE clusters :\\n  *  First on europe-west1-b which will be the `master`\\n  *  Second on europe-west1-c which will be the `slave`\\n* Firewall rules to allow clusters to communicate\\n* External DNS on each cluster to expose cassandra nodes\\n* Casskop operator on each cluster to focus on multi-casskop usage\\n\\n\\n### Environment setup\\n\\nStart to set variables needed for the instantiation : \\n\\n```sh\\n$ export CASSKOP_WORKSPACE=<path to cassandra-k8s-operateur project>\\n$ export PROJECT=<gcp project>\\n$ export SERVICE_ACCOUNT_KEY_PATH=<path to service account key>\\n$ export NAMESPACE=cassandra-demo\\n$ export DNS_ZONE_NAME=external-dns-test-gcp-trycatchlearn-fr     # -> change with your own one\\n$ export DNS_NAME=external-dns-test.gcp.trycatchlearn.fr          # -> change with your own one\\n$ export MANAGED_ZONE=tracking-pdb                                # -> change with your own one\\n```\\n\\n### Setup base infrastructure \\n\\n```sh\\n$ cd ${CASSKOP_WORKSPACE}/multi-casskop/config/samples/gke/terraform\\n$ terraform init\\n```\\n\\n#### Master provisionning\\n\\n![MultiCasskop architecture](/img/blog/2020-01-15-multicasskop_gke/multicasskop_architecture.jpeg)\\n\\nWith the master provisionning, we will deploy firewall and Cloud dns configuration :\\n\\n```sh\\n$ terraform workspace new master\\n$ terraform workspace select master\\n$ terraform apply \\\\\\n    -var-file=\\"env/master.tfvars\\" \\\\\\n    -var=\\"service_account_json_file=${SERVICE_ACCOUNT_KEY_PATH}\\" \\\\\\n    -var=\\"namespace=${NAMESPACE}\\" \\\\\\n    -var=\\"project=${PROJECT}\\" \\\\\\n    -var=\\"dns_zone_name=${DNS_ZONE_NAME}\\" \\\\\\n    -var=\\"dns_name=${DNS_NAME}\\" \\\\\\n    -var=\\"managed_zone=${MANAGED_ZONE}\\"\\n```\\n\\n#### Slave provisionning\\n\\n```sh\\n$ terraform workspace new slave\\n$ terraform workspace select slave\\n$ terraform apply \\\\\\n    -var-file=\\"env/slave.tfvars\\" \\\\\\n    -var=\\"service_account_json_file=${SERVICE_ACCOUNT_KEY_PATH}\\" \\\\\\n    -var=\\"namespace=${NAMESPACE}\\" \\\\\\n    -var=\\"project=${PROJECT}\\" \\\\\\n    -var=\\"dns_zone_name=${DNS_ZONE_NAME}\\" \\\\\\n    -var=\\"dns_name=${DNS_NAME}\\" \\\\\\n    -var=\\"managed_zone=${MANAGED_ZONE}\\"\\n```\\n\\n### Check installation \\n\\n#### Check master configuration\\n\\nNow we will check that everything is well deployed in the GKE master cluster : \\n\\n```sh\\n$ gcloud container clusters get-credentials cassandra-europe-west1-b-master --zone europe-west1-b --project ${PROJECT}\\n$ kubectl get pods -n ${NAMESPACE}\\nNAME                                          READY   STATUS    RESTARTS   AGE\\ncasskop-casskop-54c4cfcbcb-b4qxq   1/1     Running   0          4h9m\\nexternal-dns-6dd96c985-h76gh                  1/1     Running   0          4h16m\\n```\\n\\n#### Check slave configuration\\n\\nNow we will check that everything is well deployed in the GKE slave cluster : \\n\\n```sh\\n$ gcloud container clusters get-credentials cassandra-europe-west1-c-slave --zone europe-west1-c --project ${PROJECT}\\n$ kubectl get pods -n ${NAMESPACE}\\nNAME                                          READY   STATUS    RESTARTS   AGE\\ncasskop-casskop-54c4cfcbcb-sxjz7   1/1     Running   0          4m56s\\nexternal-dns-7f947c5b5b-mq7kg                 1/1     Running   0          5m46s\\n```\\n\\n#### Check DNS zone configuration\\n\\nMake a note of the nameservers that were assigned to your new zone : \\n\\n```sh\\n$ gcloud dns record-sets list \\\\\\n    --zone \\"${DNS_ZONE_NAME}\\" \\\\\\n    --name \\"${DNS_NAME}.\\" \\\\\\n    --type NS\\nNAME                                     TYPE  TTL    DATA\\nexternal-dns-test.gcp.trycatchlearn.fr.  NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.\\n```\\n\\n#### Check Firewall configuration\\n\\n@TODO : rework firewall source\\n\\n```sh\\n$ gcloud compute firewall-rules describe gke-cassandra-cluster\\nallowed:\\n- IPProtocol: udp\\n- IPProtocol: tcp\\ncreationTimestamp: \'2019-12-05T13:31:01.233-08:00\'\\ndescription: \'\'\\ndirection: INGRESS\\ndisabled: false\\nid: \'8270840333953452538\'\\nkind: compute#firewall\\nlogConfig:\\n  enable: false\\nname: gke-cassandra-cluster\\nnetwork: https://www.googleapis.com/compute/v1/projects/poc-rtc/global/networks/default\\npriority: 1000\\nselfLink: https://www.googleapis.com/compute/v1/projects/poc-rtc/global/firewalls/gke-cassandra-cluster\\nsourceRanges:\\n- 0.0.0.0/0\\ntargetTags:\\n- cassandra-cluster\\n```\\n\\n#### Check Storage Class \\n\\n```sh\\n$ kubectl get storageclasses.storage.k8s.io \\nNAME                 PROVISIONER            AGE\\nstandard (default)   kubernetes.io/gce-pd   28m\\nstandard-wait        kubernetes.io/gce-pd   24m\\n```\\n\\n## Multi casskop deployment\\n\\n### Bootstrap API access to Slave from Master\\n\\nMulti-Casskop will be deployed in `master cluster`, change your `kubectl` context to point this cluster.\\n\\nIn order to allow Multi-CassKop controller to have access to `slave` from `master`, we are going to use [kubemcsa](https://github.com/admiraltyio/multicluster-service-account/releases/tag/v0.6.1) from [admiralty](https://admiralty.io/) to be able to export secret from `slave` to `master`.\\n\\nInstall `kubemcsa` : \\n\\n```sh\\n$ export RELEASE_VERSION=v0.6.1\\n$ wget https://github.com/admiraltyio/multicluster-service-account/releases/download/${RELEASE_VERSION}/kubemcsa-linux-amd64\\n$ mkdir -p ~/tools/kubemcsa/${RELEASE_VERSION} && mv kubemcsa-linux-amd64 tools/kubemcsa/${RELEASE_VERSION}/kubemcsa\\n$ chmod +x ~/tools/kubemcsa/${RELEASE_VERSION}/kubemcsa\\n$ sudo ln -sfn  ~/tools/kubemcsa/${RELEASE_VERSION}/kubemcsa /usr/local/bin/kubemcsa\\n```\\n\\nGenerate secret for `master` : \\n\\n```sh\\n$ kubectx # Switch context on master cluster\\nSwitched to context \\"gke_<Project name>_europe-west1-b_cassandra-europe-west1-b-master\\".\\n$ kubens # Switch context on correct namespace\\nContext \\"gke_<Project name>_europe-west1-b_cassandra-europe-west1-b-master\\" modified.\\nActive namespace is \\"<Namespace>\\".\\n$ kubemcsa export --context=gke_poc-rtc_europe-west1-c_cassandra-europe-west1-c-slave --namespace ${NAMESPACE} casskop --as gke-slave-west1-c | kubectl apply -f -\\nsecret/gke-slave-west1-c created\\n```\\n\\nCheck that the secret is correctly created\\n\\n```sh\\n$ kubectl get secrets -n ${NAMESPACE}\\n...\\ngke-slave-west1-c                Opaque                                5      28s\\n```\\n\\n### Install Multi-CassKop\\n\\n@TODO : To correct once the watch object will be fixed\\n\\nAdd MultiCasskop crd on the `slave` cluster : \\n\\n```sh\\n$ kubectx # Switch context on slave cluster\\nSwitched to context \\"gke_<Project name>_europe-west1-c_cassandra-europe-west1-c-slave\\".\\n$ kubectl apply -f https://raw.githubusercontent.com/cscetbon/casskop/master/multi-casskop/config/crd/bases/multicluster_v2_cassandramulticluster_crd.yaml\\n```\\n\\nDeployment with Helm : \\n\\n```sh\\n$ kubectx # Switch context on master cluster\\nSwitched to context \\"gke_<Project name>_europe-west1-b_cassandra-europe-west1-b-master\\".\\n$ helm init --client-only\\n$ cd ${CASSKOP_WORKSPACE}\\n$ helm install --name multi-casskop oci://ghcr.io/cscetbon/multi-casskop-helm --set k8s.local=gke-master-west1-b --set k8s.remote={gke-slave-west1-c} #--no-hooks if crd already install\\n```\\n\\n### Create the MultiCasskop CRD\\n\\nNow we are ready to deploy a MultiCassKop CRD instance.\\nWe will use the example in `multi-casskop/config/samples/gke/multi-casskop-gke.yaml` :\\n\\n```sh\\n$ kubectl apply -f multi-casskop/config/samples/gke/multi-casskop-gke.yaml\\n```\\n\\n### Check multi cluster installation\\n\\nWe can see that each cluster has the required pods : \\n\\n```sh\\n$ kubectx # Switch context on master cluster\\nSwitched to context \\"gke_<Project name>_europe-west1-b_cassandra-europe-west1-b-master\\".\\n$ kubectl get pods -n ${NAMESPACE}\\nNAME                                          READY   STATUS    RESTARTS   AGE\\ncassandra-demo-dc1-rack1-0                    1/1     Running   0          8m30s\\ncasskop-casskop-54c4cfcbcb-8qncr   1/1     Running   0          34m\\nexternal-dns-6dd96c985-7jf6w                  1/1     Running   0          35m\\nmulti-casskop-67dc74dff7-z4642                1/1     Running   0          11m\\n$ kubectx # Switch context on slave cluster\\nSwitched to context \\"gke_<Project name>_europe-west1-c_cassandra-europe-west1-c-slave\\".\\n$ kubectl get pods -n ${NAMESPACE}\\nNAME                                          READY   STATUS    RESTARTS   AGE\\ncassandra-demo-dc3-rack3-0                    1/1     Running   0          6m55s\\ncassandra-demo-dc4-rack4-0                    1/1     Running   0          4m59s\\ncassandra-demo-dc4-rack4-1                    1/1     Running   0          3m20s\\ncasskop-casskop-54c4cfcbcb-sxjz7   1/1     Running   0          71m\\nexternal-dns-7f947c5b5b-mq7kg                 1/1     Running   0          72m\\n```\\n\\nIf we go in one of the created pods, we can see that nodetool see pods of both clusters : \\n\\n```sh\\n$ kubectx # Switch context on master cluster\\nSwitched to context \\"gke_<Project name>_europe-west1-b_cassandra-europe-west1-b-master\\".\\n$ kubectl exec -ti cassandra-demo-dc1-rack1-0 nodetool status\\nDatacenter: dc1\\n===============\\nStatus=Up/Down\\n|/ State=Normal/Leaving/Joining/Moving\\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\\nUN  10.52.2.3  108.62 KiB  256          49.2%             a0958905-e1fa-4410-baca-fc86f4457f1a  rack1\\nDatacenter: dc3\\n===============\\nStatus=Up/Down\\n|/ State=Normal/Leaving/Joining/Moving\\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\\nUN  10.8.3.2   74.95 KiB  256          51.5%             03f8eede-4b69-43be-a0c1-73f73470398b  rack3\\nDatacenter: dc4\\n===============\\nStatus=Up/Down\\n|/ State=Normal/Leaving/Joining/Moving\\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\\nUN  10.8.4.3   107.87 KiB  256          47.8%             1a7432e2-4ca8-4767-acdb-3b40e6ff4a57  rack4\\nUN  10.8.2.5   107.85 KiB  256          51.6%             272037ce-4146-42c1-9079-ef4561249254  rack4\\n```\\n\\n## Clean up everything\\n\\nIf you have set the `deleteCassandraCluster` to true, then when deleting the `MultiCassKop` object, it will cascade the deletion of the CassandraCluster object in the targeted k8s clusters. Then each local CassKop will delete their Cassandra clusters (else skip this step)\\n\\n```sh\\n$ kubectl delete multicasskops.db.orange.com multi-casskop-demo\\n$ helm del --purge multi-casskop\\n```\\n\\n### Cleaning slave cluster\\n\\n```sh\\n$ cd ${CASSKOP_WORKSPACE}/multi-casskop/config/samples/gke/terraform\\n$ terraform workspace select slave\\n$ terraform destroy \\\\\\n    -var-file=\\"env/slave.tfvars\\" \\\\\\n    -var=\\"service_account_json_file=${SERVICE_ACCOUNT_KEY_PATH}\\" \\\\\\n    -var=\\"namespace=${NAMESPACE}\\" \\\\\\n    -var=\\"project=${PROJECT}\\" \\\\\\n    -var=\\"dns_zone_name=${DNS_ZONE_NAME}\\" \\\\\\n    -var=\\"dns_name=${DNS_NAME}\\" \\\\\\n    -var=\\"managed_zone=${MANAGED_ZONE}\\"\\n```\\n\\n### Cleaning master cluster\\n\\nBefore running the following command, you need to clean dns records set.\\n\\n```sh\\n$ terraform workspace select master\\n$ terraform destroy \\\\\\n    -var-file=\\"env/master.tfvars\\" \\\\\\n    -var=\\"service_account_json_file=${SERVICE_ACCOUNT_KEY_PATH}\\" \\\\\\n    -var=\\"namespace=${NAMESPACE}\\" \\\\\\n    -var=\\"project=${PROJECT}\\" \\\\\\n    -var=\\"dns_zone_name=${DNS_ZONE_NAME}\\" \\\\\\n    -var=\\"dns_name=${DNS_NAME}\\" \\\\\\n    -var=\\"managed_zone=${MANAGED_ZONE}\\"\\n```"}]}')}}]);