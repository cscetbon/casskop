"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8651],{3905:(e,a,n)=>{n.d(a,{Zo:()=>c,kt:()=>k});var t=n(7294);function s(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function o(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function i(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?o(Object(n),!0).forEach((function(a){s(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function r(e,a){if(null==e)return{};var n,t,s=function(e,a){if(null==e)return{};var n,t,s={},o=Object.keys(e);for(t=0;t<o.length;t++)n=o[t],a.indexOf(n)>=0||(s[n]=e[n]);return s}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)n=o[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var l=t.createContext({}),d=function(e){var a=t.useContext(l),n=a;return e&&(n="function"==typeof e?e(a):i(i({},a),e)),n},c=function(e){var a=d(e.components);return t.createElement(l.Provider,{value:a},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},m=t.forwardRef((function(e,a){var n=e.components,s=e.mdxType,o=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),p=d(n),m=s,k=p["".concat(l,".").concat(m)]||p[m]||u[m]||o;return n?t.createElement(k,i(i({ref:a},c),{},{components:n})):t.createElement(k,i({ref:a},c))}));function k(e,a){var n=arguments,s=a&&a.mdxType;if("string"==typeof e||s){var o=n.length,i=new Array(o);i[0]=m;var r={};for(var l in a)hasOwnProperty.call(a,l)&&(r[l]=a[l]);r.originalType=e,r[p]="string"==typeof e?e:s,i[1]=r;for(var d=2;d<o;d++)i[d]=n[d];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4660:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var t=n(7462),s=(n(7294),n(3905));const o={title:"Cluster Operations",sidebar_label:"Cluster Operations"},i=void 0,r={unversionedId:"operations/cluster_operations",id:"operations/cluster_operations",title:"Cluster Operations",description:"Here is the list of operations managed by CassKop at the Cluster level which have a dedicated status in each racks.",source:"@site/docs/5_operations/1_cluster_operations.md",sourceDirName:"5_operations",slug:"/operations/cluster_operations",permalink:"/casskop/docs/operations/cluster_operations",draft:!1,editUrl:"https://github.com/cscetbon/casskop/edit/master/website/docs/5_operations/1_cluster_operations.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Cluster Operations",sidebar_label:"Cluster Operations"},sidebar:"docs",previous:{title:"Implementation architecture",permalink:"/casskop/docs/operations/implementation_architecture"},next:{title:"Pods Operations",permalink:"/casskop/docs/operations/pods_operations"}},l={},d=[{value:"Initializing",id:"initializing",level:3},{value:"With no <code>topology</code> defined",id:"with-no-topology-defined",level:4},{value:"With <code>topology</code> defined",id:"with-topology-defined",level:4},{value:"UpdateConfigMap",id:"updateconfigmap",level:3},{value:"UpdateDockerImage",id:"updatedockerimage",level:3},{value:"UpdateResources",id:"updateresources",level:3},{value:"Scaling the cluster",id:"scaling-the-cluster",level:3},{value:"ScaleUp",id:"scaleup",level:4},{value:"UpdateScaleDown",id:"updatescaledown",level:3},{value:"UpdateSeedList",id:"updateseedlist",level:3},{value:"CorrectCRDConfig",id:"correctcrdconfig",level:3},{value:"Delete a DC",id:"delete-a-dc",level:3},{value:"Kubernetes node maintenance operation",id:"kubernetes-node-maintenance-operation",level:3},{value:"The PodDisruptionBudget (PDB) protection",id:"the-poddisruptionbudget-pdb-protection",level:4},{value:"K8S host major failure: replacing a cassandra node",id:"k8s-host-major-failure-replacing-a-cassandra-node",level:3},{value:"Remove old node and create new one",id:"remove-old-node-and-create-new-one",level:4},{value:"Replace node with a new one",id:"replace-node-with-a-new-one",level:4}],c={toc:d},p="wrapper";function u(e){let{components:a,...n}=e;return(0,s.kt)(p,(0,t.Z)({},c,n,{components:a,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"Here is the list of operations managed by CassKop at the ",(0,s.kt)("strong",{parentName:"p"},"Cluster")," level which have a dedicated status in each racks."),(0,s.kt)("p",null,"Those operations are applied at the Cassandra cluster level, as opposite to Pod operations that are executed at pod\nlevel and are discussed in the next section.\nCluster Operations must only be triggered by a change made on the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster")," object."),(0,s.kt)("p",null,"Some updates in the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster")," CRD object are forbidden and will be gently dismissed by CassKop:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.dataCapacity")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.dataStorage"))),(0,s.kt)("p",null,"Some Updates in the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster")," CRD object will trigger a rolling update of the whole cluster such as :"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.resources")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.baseImage")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.version")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.configMap")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.runAsUser")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.fsGroup"))),(0,s.kt)("p",null,"Some Updates in the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster")," CRD object will not trigger change on the cluster but only in future behavior of\nCassKop :"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.autoPilot")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.autoUpdateSeedList")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.deletePVC")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.hardAntiAffinity")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.rollingPartition")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"spec.maxPodUnavailable")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"noCheckStsAreEqual"))),(0,s.kt)("p",null,"CassKop manages rolling updates for each statefulset in the cluster. Then each statefulset is making the rolling\nupdated of it's pod according to the ",(0,s.kt)("inlineCode",{parentName:"p"},"partition")," defined for each statefulset in\nthe ",(0,s.kt)("inlineCode",{parentName:"p"},"spec.topology.dc[].rack[].rollingPartition"),"."),(0,s.kt)("h3",{id:"initializing"},"Initializing"),(0,s.kt)("p",null,"The First Operation required in a Cassandra Cluster is the initialization."),(0,s.kt)("p",null,"In this Phase, the CassKop will create the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.Status")," section with an entry for each DC/Rack declared\nin the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.topology")," section."),(0,s.kt)("p",null,"We could also have Initializing status if we decided later to add some DC to our topology."),(0,s.kt)("h4",{id:"with-no-topology-defined"},"With no ",(0,s.kt)("inlineCode",{parentName:"h4"},"topology")," defined"),(0,s.kt)("p",null,"For demo we will create this CassandraCluster without topology section"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "db.orange.com/v2"\nkind: "CassandraCluster"\nmetadata:\n  name: cassandra-demo\n  labels:\n    cluster: k8s.pic\nspec:\n  nodesPerRacks: 2\n  baseImage: cassandra\n  version: latest\n  rollingPartition: 0\n  dataCapacity: "3Gi"\n  dataStorageClass: "local-storage"\n  hardAntiAffinity: false\n  deletePVC: true\n  autoPilot: false\n  config:\n    jvm-options:\n      log_gc: "true"\n  autoUpdateSeedList: true\n  resources:\n    requests:\n      cpu: \'2\'\n      memory: 2Gi\n    limits:\n      cpu: \'2\'\n      memory: 2Gi\n')),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"If no ",(0,s.kt)("inlineCode",{parentName:"p"},"topology")," has been specified, then CassKop creates the default topology and status.")),(0,s.kt)("p",null,"The default topology added by CassKop is :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"...\n  topology:\n    dc:\n    - name: dc1\n      rack:\n      - name: rack1\n")),(0,s.kt)("p",null,"The number of cassandra nodes ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.nodesPerRacks")," defines the number of cassandra nodes CassKop\nmust create in each of it's racks. In our example, there is only one default rack, so CassKop will only create 2\nnodes."),(0,s.kt)("admonition",{type:"important"},(0,s.kt)("p",{parentName:"admonition"},"with the default topology there will be no Kubernetes NodesAffinity to spread the Cassandra nodes on the\ncluster. In this case, CassKop will only create one Rack and one DC for Cassandra. It is not recommended as you may\nlose data in case of hardware failure")),(0,s.kt)("p",null,"When Initialization has ended you should have a Status similar to :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: Initializing\n        endTime: 2018-09-18T15:10:51Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: Initializing\n  lastClusterActionStatus: Done\n  phase: Running\n  seedlist:\n  - cassandra-demo-dc1-rack1-0.cassandra-demo-dc1-rack1.cassandra-test\n  - cassandra-demo-dc1-rack1-1.cassandra-demo-dc1-rack1.cassandra-test\n")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The Status of the ",(0,s.kt)("inlineCode",{parentName:"li"},"dc1-rack1")," is ",(0,s.kt)("inlineCode",{parentName:"li"},"Initializing=Done")),(0,s.kt)("li",{parentName:"ul"},"The Status of the Cluster is ",(0,s.kt)("inlineCode",{parentName:"li"},"Initializing=Done")),(0,s.kt)("li",{parentName:"ul"},"The phase is ",(0,s.kt)("inlineCode",{parentName:"li"},"Running")," which means that each Rack has the desired amount of Nodes.")),(0,s.kt)("p",null,"We asked 2 ",(0,s.kt)("inlineCode",{parentName:"p"},"nodesPerRacks")," and we have one default rack, so we ended with 2 Cassandra nodes in our cluster."),(0,s.kt)("p",null,"The Cassandra ",(0,s.kt)("inlineCode",{parentName:"p"},"seedlist")," has been initialized and stored in the CassandraCluster.status.seedlist`. It has also been\nconfigured in each of the Cassandra Pods."),(0,s.kt)("p",null,"We can also confirm that Cassandra knows about the DC and Rack name we have deployed :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-console"},"$ kubectl exec -ti cassandra-demo-dc1-rack1-0 nodetool status\nDatacenter: dc1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.18.72.28   65.86 KiB  32           100.0%            fdc1a9e9-c5c3-4169-ae47-e6843efa096d  rack1\nUN  172.18.120.12  65.86 KiB  32           100.0%            509ca725-fbf9-422f-a8e0-5e2a55474f70  rack1\n")),(0,s.kt)("h4",{id:"with-topology-defined"},"With ",(0,s.kt)("inlineCode",{parentName:"h4"},"topology")," defined"),(0,s.kt)("p",null,"In this example, I added a topology defining 2 Cassandra DC and 3 racks in total"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "db.orange.com/v2"\nkind: "CassandraCluster"\nmetadata:\n  name: cassandra-demo\n  labels:\n    cluster: k8s.pic\nspec:\n  nodesPerRacks: 2\n  baseImage: cassandra\n  version: latest\n  rollingPartition: 0\n  dataCapacity: "3Gi"\n  dataStorageClass: "local-storage"\n  hardAntiAffinity: false\n  deletePVC: true\n  autoPilot: true\n  autoUpdateSeedList: true\n  resources:\n    requests:\n      cpu: \'2\'\n      memory: 2Gi\n    limits:\n      cpu: \'2\'\n      memory: 2Gi\n  topology:\n    dc:\n      - name: dc1\n        labels:\n          failure-domain.beta.kubernetes.io/region: europe-west1\n        rack:\n          - name: rack1\n            labels:\n              failure-domain.beta.kubernetes.io/zone: europe-west1-b\n          - name: rack2\n            labels:\n              failure-domain.beta.kubernetes.io/zone: europe-west1-c\n      - name: dc2\n        nodesPerRacks: 3\n        config:\n          cassandra-yaml:\n            num_tokens: 32\n        labels:\n          failure-domain.beta.kubernetes.io/region: europe-west1\n        rack:\n          - name: rack1\n            labels:\n              failure-domain.beta.kubernetes.io/zone: europe-west1-d\n')),(0,s.kt)("p",null,"With this topology section I also references some ",(0,s.kt)("strong",{parentName:"p"},"Kubernetes nodes labels"),"\nwhich will be used to spread the Cassandra\nnodes on each Racks on different groups of Kubernetes servers."),(0,s.kt)("admonition",{type:"note"},(0,s.kt)("p",{parentName:"admonition"},"We can see here that we can give specific configuration for the number of pods in the dc2 (",(0,s.kt)("inlineCode",{parentName:"p"},"nodesPerRacks: 3"),")\nWe also allow to configure Cassandra pods with different num_tokens confioguration for each dc using the appropriate\nparameter in the config.")),(0,s.kt)("p",null,"CassKop will create a statefulset for each Rack, and start creating the\nCassandra Cluster, starting by nodes from the Rack 1.\nWhen CassKop will end operations on Rack1, it will process the next rack and so on."),(0,s.kt)("p",null,"The status may be similar to :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: Initializing\n        status: Ongoing\n      phase: Initializing\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: Initializing\n        status: Ongoing\n      phase: Initializing\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: Initializing\n        status: Ongoing\n      phase: Initializing\n      podLastOperation: {}\n  lastClusterAction: Initializing\n  lastClusterActionStatus: Ongoing\n  phase: Initializing\n  seedlist:\n  - cassandra-demo-dc1-rack1-0.cassandra-demo-dc1-rack1.cassandra-test\n  - cassandra-demo-dc1-rack1-1.cassandra-demo-dc1-rack1.cassandra-test\n  - cassandra-demo-dc1-rack2-0.cassandra-demo-dc1-rack2.cassandra-test\n  - cassandra-demo-dc2-rack1-0.cassandra-demo-dc2-rack1.cassandra-test\n  - cassandra-demo-dc2-rack1-1.cassandra-demo-dc2-rack1.cassandra-test\n  - cassandra-demo-dc2-rack1-2.cassandra-demo-dc2-rack1.cassandra-test\n")),(0,s.kt)("p",null,"The creation of the cluster is ongoing.\nWe can see that, regarding the Cluster Topology, CassKop has created the SeedList."),(0,s.kt)("admonition",{type:"tip"},(0,s.kt)("p",{parentName:"admonition"},"CassKop compute a seedlist with 3 nodes in each datacenter (if possible). The Cassandra seeds are always the\nfirst Cassandra nodes of a statefulset (starting with index 0).")),(0,s.kt)("p",null,"When all racks are in status done, then the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.status.lastClusterActionStatus")," is changed to ",(0,s.kt)("inlineCode",{parentName:"p"},"Done"),"."),(0,s.kt)("p",null,"We can see that internally Cassandra also knows the desired topology :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl exec -ti cassandra-demo-dc1-rack1-0 nodetool status\nDatacenter: dc1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.18.112.6   126.05 KiB  32           38.5%             1512da3c-f6b2-469f-95d1-2d060043777a  rack1\nUN  172.18.64.10   137.08 KiB  32           32.0%             8149054f-4bc3-4093-a6ef-80910c018122  rack2\nUN  172.18.88.9    154.54 KiB  32           30.2%             dbe44aa6-6763-4bc1-825a-9ea7d21690e3  rack2\nUN  172.18.120.15  119.88 KiB  32           33.7%             c87e858d-66a8-4544-9d28-718a1f94955b  rack1\nDatacenter: dc2\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.18.72.8    119.65 KiB  32           26.9%             8688abd3-08b6-44e0-8805-05bd3650eea6  rack1\nUN  172.18.104.8   153.08 KiB  32           38.8%             62adf02d-8c55-4d95-a459-45b1c9c3aa91  rack1\n")),(0,s.kt)("h3",{id:"updateconfigmap"},"UpdateConfigMap"),(0,s.kt)("p",null,"You can find in the ",(0,s.kt)("a",{parentName:"p",href:"/casskop/docs/configuration_deployment/cassandra_configuration#configuration-override-using-configmap"},"cassandra-configuration")," section how you can use\nthe ",(0,s.kt)("inlineCode",{parentName:"p"},"spec.configMap")," parameter."),(0,s.kt)("admonition",{type:"important"},(0,s.kt)("p",{parentName:"admonition"},"Actually CassKop doesn't monitor changes inside the ConfigMap. If you want to change a parameter in a\nfile in the current configMap, you must create a new configMap with the updated version, and then ask CassKop to use\nthe new configmap name.")),(0,s.kt)("p",null,"If we add/change/remove the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.configMapName")," then CassKop will start a RollingUpdate of each\nCassandraNodes in each Racks, starting from the first Rack defined in the ",(0,s.kt)("inlineCode",{parentName:"p"},"topology"),"."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "db.orange.com/v2"\nkind: "CassandraCluster"\nmetadata:\n  name: cassandra-demo\n  labels:\n    cluster: k8s.pic\nspec:\n  nodesPerRacks: 2\n  baseImage: cassandra\n  version: latest\n  rollingPartition: 0\n  dataCapacity: "3Gi"\n  dataStorageClass: "local-storage"\n  hardAntiAffinity: false\n  deletePVC: true\n  autoPilot: true\n  autoUpdateSeedList: true\n  configMapName: cassandra-configmap-v1\n  ...\n')),(0,s.kt)("p",null,"First we need to create the configmap exemple:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f config/samples/cassandra-configmap-v1.yaml\n")),(0,s.kt)("p",null,"Then we apply the changes in the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster"),"."),(0,s.kt)("p",null,"We can see the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.Status")," updated by CassKop"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateConfigMap\n        startTime: 2018-09-21T12:24:24Z\n        status: Ongoing\n      phase: Pending\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: Initializing\n        endTime: 2018-09-21T10:33:10Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: Initializing\n        endTime: 2018-09-21T10:34:47Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: UpdateConfigMap\n  lastClusterActionStatus: Ongoing\n")),(0,s.kt)("admonition",{type:"note"},(0,s.kt)("p",{parentName:"admonition"},"CassKop won't make a rolling update on the next rack until the status of the current rack becomes",(0,s.kt)("inlineCode",{parentName:"p"},"Done"),'.\nThe Operation is processing "rack per rack".')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateConfigMap\n        endTime: 2018-09-21T12:26:10Z\n        startTime: 2018-09-21T12:24:24Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateConfigMap\n        endTime: 2018-09-21T12:27:25Z\n        startTime: 2018-09-21T12:26:10Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: UpdateConfigMap\n        startTime: 2018-09-21T12:27:27Z\n        status: Ongoing\n      phase: Pending\n      podLastOperation: {}\n  lastClusterAction: UpdateConfigMap\n  lastClusterActionStatus: Ongoing\n")),(0,s.kt)("h3",{id:"updatedockerimage"},"UpdateDockerImage"),(0,s.kt)("p",null,"CassKop allows you to change the Cassandra docker image and gracefully redeploy your whole cluster."),(0,s.kt)("p",null,"If we change the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.baseImage")," and or ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.version"),"  CassKop will start to\nperform a RollingUpdate on the whole cluster (for each racks sequentially, in order to change the version of the\nCassandra Docker Image on all nodes."),(0,s.kt)("p",null,"You can change the docker image used to :"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"change the version of Cassandra"),(0,s.kt)("li",{parentName:"ul"},"change the version of Java"),(0,s.kt)("li",{parentName:"ul"},"Change some configuration parameters for cassandra or jvm if you don't overwrite them with a ConfigMap")),(0,s.kt)("p",null,"The status may be similar to:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateDockerImage\n        startTime: 2018-09-18T16:08:59Z\n        status: Ongoing\n      phase: Pending\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: Initializing\n        endTime: 2018-09-18T16:05:51Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: Initializing\n        endTime: 2018-09-18T16:07:52Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: UpdateDockerImage\n  lastClusterActionStatus: Ongoing\n  phase: Pending\n  seedlist:\n  - cassandra-demo-dc1-rack1-0.cassandra-demo-dc1-rack1.cassandra-test\n  - cassandra-demo-dc1-rack1-1.cassandra-demo-dc1-rack1.cassandra-test\n  - cassandra-demo-dc1-rack2-0.cassandra-demo-dc1-rack2.cassandra-test\n  - cassandra-demo-dc2-rack1-0.cassandra-demo-dc2-rack1.cassandra-test\n  - cassandra-demo-dc2-rack1-1.cassandra-demo-dc2-rack1.cassandra-test\n  - cassandra-demo-dc2-rack1-2.cassandra-demo-dc2-rack1.cassandra-test\n")),(0,s.kt)("p",null,"We can see that CassKop has started to Update the ",(0,s.kt)("inlineCode",{parentName:"p"},"dc1-rack1")," and it has changed the ",(0,s.kt)("inlineCode",{parentName:"p"},"lastClusterAction")," and\n",(0,s.kt)("inlineCode",{parentName:"p"},"lastClusterStatus")," accordingly."),(0,s.kt)("p",null,"Once it has finished the first rack, then it processes the next one:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateDockerImage\n        endTime: 2018-09-18T16:10:51Z\n        startTime: 2018-09-18T16:08:59Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateDockerImage\n        startTime: 2018-09-18T16:10:51Z\n        status: Ongoing\n      phase: Pending\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: Initializing\n        endTime: 2018-09-18T16:07:52Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: UpdateDockerImage\n  lastClusterActionStatus: Ongoing\n")),(0,s.kt)("p",null,"And when all racks are Done:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateDockerImage\n        endTime: 2018-09-18T16:10:51Z\n        startTime: 2018-09-18T16:08:59Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateDockerImage\n        endTime: 2018-09-18T16:12:42Z\n        startTime: 2018-09-18T16:10:51Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: UpdateDockerImage\n        endTime: 2018-09-18T16:14:52Z\n        startTime: 2018-09-18T16:12:42Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: UpdateDockerImage\n  lastClusterActionStatus: Done\n  phase: Running\n")),(0,s.kt)("p",null,"This provides a Central view to monitor what is happening on the Cassandra Cluster."),(0,s.kt)("h3",{id:"updateresources"},"UpdateResources"),(0,s.kt)("p",null,"CassKop allows you to configure your Cassandra's pods resources (memory and cpu)."),(0,s.kt)("p",null,"If we change the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.resources"),", then CassKop will start to make a RollingUpdate on the whole\ncluster (for each racks sequentially) to change the version of the Cassandra Docker Image on all nodes."),(0,s.kt)("admonition",{title:"See section",type:"tip"},(0,s.kt)("p",{parentName:"admonition"},(0,s.kt)("a",{parentName:"p",href:"/casskop/docs/configuration_deployment/cassandra_cluster#resource-limits-and-requests"},"Resource limits and requets"))),(0,s.kt)("p",null,"For example, to increase Memory/CPU requests and/or limits:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"    requests:\n      cpu: '2'\n      memory: 3Gi\n    limits:\n      cpu: '2'\n      memory: 3Gi\n")),(0,s.kt)("p",null,"Then CassKop should output the status:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateResources\n        startTime: 2018-09-21T15:28:43Z\n        status: Ongoing\n      phase: Pending\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateResources\n        startTime: 2018-09-21T15:28:43Z\n        status: ToDo\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: UpdateResources\n        startTime: 2018-09-21T15:28:43Z\n        status: ToDo\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: UpdateResources\n  lastClusterActionStatus: Ongoing\n")),(0,s.kt)("p",null,"We can see that it has staged the ",(0,s.kt)("inlineCode",{parentName:"p"},"UpdateResources")," action in all racks (",(0,s.kt)("inlineCode",{parentName:"p"},"status=ToDo"),") and has started the action in\nthe first rack (",(0,s.kt)("inlineCode",{parentName:"p"},"status=Ongoing"),"). Once ",(0,s.kt)("inlineCode",{parentName:"p"},"Done")," it will follow with next rack, and so on."),(0,s.kt)("p",null,"Upon completion, the status may look like :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateResources\n        endTime: 2018-09-21T15:30:31Z\n        startTime: 2018-09-21T15:28:43Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateResources\n        endTime: 2018-09-21T15:32:12Z\n        startTime: 2018-09-21T15:30:32Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: UpdateResources\n        endTime: 2018-09-21T15:34:07Z\n        startTime: 2018-09-21T15:32:13Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n  lastClusterAction: UpdateResources\n  lastClusterActionStatus: Done\n")),(0,s.kt)("h3",{id:"scaling-the-cluster"},"Scaling the cluster"),(0,s.kt)("p",null,"The Scaling of the Cluster is managed through the nodesPerRacks parameters and through the number of Dcs and Racks\ndefined in the Topology section."),(0,s.kt)("p",null,"See section ",(0,s.kt)("a",{parentName:"p",href:"/casskop/docs/configuration_deployment/cassandra_configuration#nodes-per-rack"},"NodesPerRacks")),(0,s.kt)("admonition",{type:"note"},(0,s.kt)("p",{parentName:"admonition"},"if the ScaleUp (or the ScaleDown) may change the SeedList and if ",(0,s.kt)("inlineCode",{parentName:"p"},"spec.autoUpdateSeedList")," is set to ",(0,s.kt)("inlineCode",{parentName:"p"},"true"),"\nthen CassKop will program a new operation : ",(0,s.kt)("inlineCode",{parentName:"p"},"UpdateSeedList")," which will trigger a rollingUpdate to apply the new\nseedlist on all nodes, once the Scaling is done.")),(0,s.kt)("h4",{id:"scaleup"},"ScaleUp"),(0,s.kt)("p",null,"CassKop allows you to Scale Up your Cassandra cluster."),(0,s.kt)("p",null,"There is a global parameter ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.nodesPerRacks")," which specify the number of Cassandra nodes we want in\na rack."),(0,s.kt)("p",null,"It is possible to surcharge this for a particular DC in the ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.topology.dc[<idx>].nodesPerRacks")),(0,s.kt)("p",null,"Example:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"  topology:\n    dc:\n      - name: dc1\n        rack:\n          - name: rack1\n          - name: rack2\n      - name: dc2\n        nodesPerRacks: 3        <--- We increase by one this value\n        rack:\n          - name: rack1\n")),(0,s.kt)("p",null,"In this case, we ask to ScaleUp nodes of second DC ",(0,s.kt)("inlineCode",{parentName:"p"},"dc2")),(0,s.kt)("p",null,"CassKop takes into account the new target, and starts applying modifications in the cluster :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"...\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        status: Configuring\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        status: Configuring\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: ScaleUp\n        startTime: 2018-09-27T15:02:21Z\n        status: Configuring\n      phase: Pending\n  lastClusterAction: ScaleUp\n  lastClusterActionStatus: Ongoing\n ...\n")),(0,s.kt)("p",null,"We can see that CassKop:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"has started the ",(0,s.kt)("inlineCode",{parentName:"li"},"ScaleUp")," action in ",(0,s.kt)("inlineCode",{parentName:"li"},"dc2-rack1")),(0,s.kt)("li",{parentName:"ul"},"has found that the SeedList must be updated, and because the autoUpdateSeedList=true it has staged\n(",(0,s.kt)("inlineCode",{parentName:"li"},"status=Configuring"),") the UpdateSeedList operation for ",(0,s.kt)("inlineCode",{parentName:"li"},"dc1-rack1")," and ",(0,s.kt)("inlineCode",{parentName:"li"},"dc1-rack2"))),(0,s.kt)("p",null,"When CassKop ends the ScaleUp action in the ",(0,s.kt)("inlineCode",{parentName:"p"},"dc2-rack1")," then it will also stage this rack with ",(0,s.kt)("inlineCode",{parentName:"p"},"UpdateSeedList=Configuring"),".\nOnce all racks are in this state, CassKop will turn each Rack in status ",(0,s.kt)("inlineCode",{parentName:"p"},"UpdateSeedList=ToDo"),", meaning that it can\nstart the operation."),(0,s.kt)("p",null,"Starting from then, CassKop will iterate on each rack one after the other and get status :"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"UpdateSeedList=Ongoing")," meaning that it is currently doing a rolling update on the Rack to update the SeedList seting\nalso sets the ",(0,s.kt)("inlineCode",{parentName:"li"},"startTime"),"."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"UpdateSeedList=Done")," meaning that the operation is done. (then, it sets the ",(0,s.kt)("inlineCode",{parentName:"li"},"endTime"),")")),(0,s.kt)("p",null,"See evolution of status:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        endTime: 2018-09-27T15:05:00Z\n        startTime: 2018-09-27T15:03:13Z\n        status: Done\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        startTime: 2018-09-27T15:03:13Z\n        status: Ongoing\n      phase: Pending\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        status: ToDo\n      phase: Running\n  lastClusterAction: UpdateSeedList\n  lastClusterActionStatus: Finalizing\n  phase: Pending\n")),(0,s.kt)("p",null,"Here is the final topology seen from nodetool :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl  exec -ti cassandra-demo-dc1-rack1-0 nodetool status\nDatacenter: dc1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.18.88.6    211.95 KiB  32           27.4%             dbe44aa6-6763-4bc1-825a-9ea7d21690e3  rack2\nUN  172.18.112.5   231.49 KiB  32           29.2%             1512da3c-f6b2-469f-95d1-2d060043777a  rack1\nUN  172.18.64.10   188.36 KiB  32           27.6%             8149054f-4bc3-4093-a6ef-80910c018122  rack2\nUN  172.18.120.14  237.62 KiB  32           29.8%             c87e858d-66a8-4544-9d28-718a1f94955b  rack1\nDatacenter: dc2\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.18.72.8    228.54 KiB  32           22.0%             8688abd3-08b6-44e0-8805-05bd3650eea6  rack1\nUN  172.18.104.8   212.34 KiB  32           32.2%             62adf02d-8c55-4d95-a459-45b1c9c3aa91  rack1\nUN  172.18.88.9    148.34 KiB  32           31.7%             fecdfb5d-3ad4-4204-8ca5-cc7f1c4c19c4  rack1\n")),(0,s.kt)("p",null,"Note that nodetool prints IP of nodes while kubernetes works with names :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get pods -o wide -l app=cassandracluster\nNAME                         READY     STATUS    RESTARTS   AGE       IP              NODE      NOMINATED NODE\ncassandra-demo-dc1-rack1-0   1/1       Running   0          14m       172.18.112.5    node006   <none>\ncassandra-demo-dc1-rack1-1   1/1       Running   0          15m       172.18.120.14   node003   <none>\ncassandra-demo-dc1-rack2-0   1/1       Running   0          13m       172.18.88.6     node005   <none>\ncassandra-demo-dc1-rack2-1   1/1       Running   0          13m       172.18.64.10    node004   <none>\ncassandra-demo-dc2-rack1-0   1/1       Running   0          10m       172.18.72.8     node008   <none>\ncassandra-demo-dc2-rack1-1   1/1       Running   0          11m       172.18.104.8    node007   <none>\ncassandra-demo-dc2-rack1-2   1/1       Running   0          12m       172.18.88.9     node005   <none>\n")),(0,s.kt)("p",null,"After the ScaleUp has finished, CassKop must execute a cassandra ",(0,s.kt)("inlineCode",{parentName:"p"},"cleanup")," on each nodes of the Cluster.\nThis can be manually triggered by setting appropriate labels on each Pods."),(0,s.kt)("p",null,"CassKop can automate this if ",(0,s.kt)("inlineCode",{parentName:"p"},"spec.autoPilot")," is true by setting the labels on each Pods of the cluster with a ToDo\nstate and then find thoses pods to sequentially execute thoses actions."),(0,s.kt)("p",null,"See podOperation ",(0,s.kt)("a",{parentName:"p",href:"/casskop/docs/operations/pods_operations#operationcleanup"},"Cleanup"),"!!"),(0,s.kt)("h3",{id:"updatescaledown"},"UpdateScaleDown"),(0,s.kt)("p",null,"For ScaleDown, CassKop must perform a clean cassandra ",(0,s.kt)("inlineCode",{parentName:"p"},"decommission")," prior to actually scale down the cluster at\nKubernetes level."),(0,s.kt)("p",null,"Actually, this is done through CassKop asking the decommission through a jolokia call and waiting for it to be\nperformed (cassandra node status = decommissionned) before updating kubernetes statefulset (removing the pod)."),(0,s.kt)("admonition",{type:"important"},(0,s.kt)("p",{parentName:"admonition"},"If we ask to scale down more than 1 node at a time, then CassKop will iterate on a single scale down\nuntil it reaches the requested number of nodes."),(0,s.kt)("p",{parentName:"admonition"},"Also CassKop will refuse a scaledown to 0 for a DC if there still have some data replicated to it.")),(0,s.kt)("p",null,"To launch a ScaleDown, we simply need to decrease the value of nodesPerRacks."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"  topology:\n    dc:\n      - name: dc1\n        rack:\n          - name: rack1\n          - name: rack2\n      - name: dc2\n        nodesPerRacks: 2        <--- Get back to 2\n        rack:\n          - name: rack1\n")),(0,s.kt)("p",null,"We can see in the below example that:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"It has started the ",(0,s.kt)("inlineCode",{parentName:"li"},"ScaleDown")," action in ",(0,s.kt)("inlineCode",{parentName:"li"},"dc2-rack1")),(0,s.kt)("li",{parentName:"ul"},"CassKop has found that the SeedList must be updated, and it has staged (",(0,s.kt)("inlineCode",{parentName:"li"},"status=ToDo"),") it for ",(0,s.kt)("inlineCode",{parentName:"li"},"dc1-rack1")," and\n",(0,s.kt)("inlineCode",{parentName:"li"},"dc1-rack2"))),(0,s.kt)("p",null,"When CassKop completes the ScaleDown in the ",(0,s.kt)("inlineCode",{parentName:"p"},"dc2-rack1")," then it will stage it also with ",(0,s.kt)("inlineCode",{parentName:"p"},"UpdateSeedList=ToDo")," Once all\nracks are in this state, CassKop will turn each Rack in status ",(0,s.kt)("inlineCode",{parentName:"p"},"UpdateSeedList=Ongoing")," meaning that it can start the\noperation, it also set the ",(0,s.kt)("inlineCode",{parentName:"p"},"startTime")),(0,s.kt)("p",null,"Then, CassKop will iterate on each rack one after the other and get status :"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"UpdateSeedList=Finalizing")," meaning that it is currently doing a rolling update on the Rack to update the SeedList"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"UpdateSeedList=Done")," meaning that the operation is done. Then, it sets the ",(0,s.kt)("inlineCode",{parentName:"li"},"endTime"),".")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        status: ToDo\n      phase: Running\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        status: ToDo\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: ScaleDown\n        startTime: 2018-09-27T15:22:23Z\n        status: Ongoing\n      phase: Running\n      podLastOperation:\n        Name: decommission\n        pods:\n        - cassandra-demo-dc2-rack1-2\n        startTime: 2018-09-27T15:22:23Z\n        status: Ongoing\n  lastClusterAction: ScaleDown\n  lastClusterActionStatus: Ongoing\n")),(0,s.kt)("p",null,"When ",(0,s.kt)("inlineCode",{parentName:"p"},"ScaleDown=Done")," CassKop will start the UpdateSeedList operation."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"status:\n  cassandraRackStatus:\n    dc1-rack1:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        startTime: 2018-09-27T15:23:54Z\n        status: Finalizing\n      phase: Pending\n      podLastOperation: {}\n    dc1-rack2:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        startTime: 2018-09-27T15:23:54Z\n        status: Ongoing\n      phase: Running\n      podLastOperation: {}\n    dc2-rack1:\n      cassandraLastAction:\n        Name: UpdateSeedList\n        startTime: 2018-09-27T15:23:54Z\n        status: Ongoing\n      phase: Running\n      podLastOperation:\n        Name: decommission\n        endTime: 2018-09-27T15:23:51Z\n        podsOK:\n        - cassandra-demo-dc2-rack1-2\n        startTime: 2018-09-27T15:22:23Z\n        status: Done\n  lastClusterAction: UpdateSeedList\n  lastClusterActionStatus: Finalizing\n  phase: Pending\n")),(0,s.kt)("p",null,"It shows also that ",(0,s.kt)("inlineCode",{parentName:"p"},"podLastOperation")," ",(0,s.kt)("inlineCode",{parentName:"p"},"decommission")," is ",(0,s.kt)("inlineCode",{parentName:"p"},"Done"),". CassKop will then rollingUpdate all racks one by one\nin order to update the Cassandra seedlist."),(0,s.kt)("h3",{id:"updateseedlist"},"UpdateSeedList"),(0,s.kt)("p",null,"The UpdateSeedList is done automatically by CassKop when the parameter\n",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster.spec.autoUpdateSeedList")," is true (default)."),(0,s.kt)("p",null,"See ",(0,s.kt)("a",{parentName:"p",href:"#scaleup"},"ScaleUp")," and ",(0,s.kt)("a",{parentName:"p",href:"#updatescaledown"},"ScaleDown"),"."),(0,s.kt)("h3",{id:"correctcrdconfig"},"CorrectCRDConfig"),(0,s.kt)("p",null,"The CRD ",(0,s.kt)("inlineCode",{parentName:"p"},"CassandraCluster")," is used to define your cluster configuration. Some fields can't be updated in a kubernetes\nclusters. Some fields are taken from the CRD to configure thoses objects, and to be sure we don't update them (to\nprevent kubernetes objects in errors), we have configure CassKop to simply ignore/revert unauthorized changed to the\nCRD."),(0,s.kt)("p",null,"Example With this CRD deployed :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  nodesPerRacks: 2\n  baseImage: cassandra\n  version: latest\n  imagePullSecret:\n    name: advisedev # To authenticate on docker registry\n  rollingPartition: 0\n  dataCapacity: "3Gi"                  <-- can\'t be changed\n  dataStorageClass: "local-storage"    <-- can\'t be changed\n  hardAntiAffinity: false\n  deletePVC: true\n  autoPilot: true\n  autoUpdateSeedList: true\n')),(0,s.kt)("p",null,"If we try to update the ",(0,s.kt)("inlineCode",{parentName:"p"},"dataCapacity")," or ",(0,s.kt)("inlineCode",{parentName:"p"},"dataStorageClass")," nothing will happen. And we could see thoses messages in\nthe logs of CassKop :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-logs"},'time="2018-09-27T17:44:13+02:00" level=warning msg="[cassandra-demo]: CassKop has refused the changed on DataCapacity from [3Gi] to NewValue[4Gi]"\ntime="2018-09-27T17:44:35+02:00" level=warning msg="[cassandra-demo]: CassKop has refused the changed on DataStorageClass from [local-storage] to NewValue[local-storag]"\n')),(0,s.kt)("p",null,"If you performed the modification by updating your local CRD file and apply it with kubectl you must revert to the old\nvalue."),(0,s.kt)("h3",{id:"delete-a-dc"},"Delete a DC"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Prior to delete a DC, you must have ScaleDown to 0 all the Racks, if not, CassKop will refuse and correct the CRD."),(0,s.kt)("li",{parentName:"ul"},"Prior to scaleDown to 0 CassKop will ensure that there are no more data replicated to the DC, if not, CassKop\nwill refuse and correct the CRD.\nBecause CassKop wants that we have the same amounts of pods in all racks, we decided that we would't allow to remove\nonly a rack. This will be revert too.")),(0,s.kt)("admonition",{type:"important"},(0,s.kt)("p",{parentName:"admonition"},"You must ScaleDown to 0 before you remoove a DC\nYou must change replication factor before doing a ScaleDown to 0 for a DC")),(0,s.kt)("h3",{id:"kubernetes-node-maintenance-operation"},"Kubernetes node maintenance operation"),(0,s.kt)("p",null,"In a normal production environment, CassKop will have spread it's Cassandra pods on differents k8s nodes. If the team\nin charge of the machines needs to make some operations on a host they can make a drain."),(0,s.kt)("p",null,"The Kubernetes drain command will ask the scheduler to make an eviction for all pods on the current nodes, and for many\nworkloads k8s will reschedule them on other machines. In the case of CassKop cassandra pods, they won't be scheduled\non another host, because they uses local-storage and are stick to a specific host thanks to the PersistentVolumeClaim\nkubernetes object."),(0,s.kt)("p",null,"Example: we drain the node008 for a maintenance operation."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl drainnode node008 --ignore-daemonsets --delete-local-data\n")),(0,s.kt)("p",null,"All pods will be evicted, thoses who can will be rescheduled on another hosts.\nOur Cassandra pod won't we able to be schedule elsewhere due to the PVS, and we can see this messages in the k8s events :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-logs"},"0s    Warning   FailedScheduling   Pod   0/8 nodes are available: 1 node(s) were unschedulable, 2 node(s) had taints\nthat the pod didn't tolerate, 5 node(s) had volume node affinity conflict.\n")),(0,s.kt)("p",null,"It explain that 1 node is unshedulable, this is the one we just drain. the 5 other nodes can't be scheduled by our pod\nbecause they have volume node affinity conflict ()our pods have an affinity on node008)."),(0,s.kt)("p",null,"Once the team have finished their maintenance operation they can bring back the host into the kubernetes cluster. From\nthen, k8s will be able to reshedule back the cassandra pod into the cluster so that it can re-join the ring."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl uncordon node008\nnode/node008 uncordoned\n")),(0,s.kt)("p",null,"Immediately the pending pod is rescheduled and started on the host.\nIf the time of interruption was not too long there is nothing more to do, the node will join the ring and re-synchronise\nwith the cluster. If the time was too long, then it may be needed to schedule some PodOperations that you will find in\nnexts sections of this document."),(0,s.kt)("h4",{id:"the-poddisruptionbudget-pdb-protection"},"The PodDisruptionBudget (PDB) protection"),(0,s.kt)("p",null,"If a k8s admin ask to drain a node, this may not been allowed by the cassandracluster regarding it's current state and\nthe configuration of its PDB (usually only 1 nodes allowed to be in disruption)."),(0,s.kt)("p",null,"Example :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'$ kubectl drainnode node008 --ignore-daemonsets --delete-local-data\nerror when evicting pod "cassandra-demo-dc2-rack1-0" (will retry after 5s): Cannot evict pod as it would violate the pod\'s disruption budget.\n')),(0,s.kt)("p",null,"The node008 will be flagged as SchedulingDisabled, so that it won't take new workload. It will evict all possible pods,\nbut if there was an ongoing disruption on the current Cassandra cluster, it won't be allowed to evict the cassandra pod."),(0,s.kt)("p",null,"Example of a PDB :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app: cassandracluster\n    cassandracluster: cassandra-test\n    cluster: k8s.pic\n  name: cassandra-test\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: cassandracluster\n      cassandracluster: cassandra-test\n      cluster: k8s.pic\nstatus:\n  currentHealthy: 13\n  desiredHealthy: 13\n  disruptedPods: null\n  disruptionsAllowed: 0\n  expectedPods: 14\n  observedGeneration: 1\n")),(0,s.kt)("p",null,"In this example we see that we allowed only 1 Pod unavailable, and on our cluster we wants to have 14 pods and we only\nhave 13 healthy, that's why the PDB won't allow the eviction of an additional pod."),(0,s.kt)("p",null,"To be able to continue, we need to wait or to make appropriate actions so that the Cassandra cluster won't have any\nunavailable nodes."),(0,s.kt)("h3",{id:"k8s-host-major-failure-replacing-a-cassandra-node"},"K8S host major failure: replacing a cassandra node"),(0,s.kt)("p",null,"In the case of a major host failure, it may not be possible to bring back the node to life. We can in this case\nconsider that our cassandra node is lost and we will want to replace it on another host."),(0,s.kt)("p",null,"In this case we may have 2 solutions that will require some manual actions :"),(0,s.kt)("h4",{id:"remove-old-node-and-create-new-one"},"Remove old node and create new one"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"In this case we will use CassKop client to schedule a cassandra removenode for the failing node.")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl casskop remove --pod <pod_name> [--previous-ip <previous_ip_pod>] {--from-pod <pod_name> | --crd <crd_name>}\n")),(0,s.kt)("p",null,"This will trigger the PodOperation removenode by setting the appropriate labels on a cassandra Pod."),(0,s.kt)("ol",{start:2},(0,s.kt)("li",{parentName:"ol"},"Once the node is properly removed, we can free the link between the Pod and the failing host by removing the\nassociated PodDisruptionBudget")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl delete pvc data-cassandra-test-dc1-rack2-1\n")),(0,s.kt)("p",null,"This will allow Kubernetes to reschedule the Pod on another free host."),(0,s.kt)("ol",{start:3},(0,s.kt)("li",{parentName:"ol"},"Once the node is back in the cluster we need to apply a cleanup on all nodes")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl casskop cleanup start\n")),(0,s.kt)("p",null,"you can pause the cleanup and check status with"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl casskop cleanup pause\nkubectl casskop cleanup status\n")),(0,s.kt)("h4",{id:"replace-node-with-a-new-one"},"Replace node with a new one"),(0,s.kt)("p",null,"In some cases It may be useful to prefer to replace the node. Because we use a statefulset to deploy cassandra pods,\nby definition all pods are identical and we couldn't execute specific actions on a specific node at startup."),(0,s.kt)("p",null,"For that CassKop provide the ability to execute a ",(0,s.kt)("inlineCode",{parentName:"p"},"pre_run.sh")," script that can be change using the CRD ConfigMap."),(0,s.kt)("p",null,"To see how to use the configmap see ",(0,s.kt)("a",{parentName:"p",href:"/casskop/docs/configuration_deployment/cassandra_configuration#configuration-override-using-configmap"},"Overriding Configuration using configMap")),(0,s.kt)("p",null,"for example If we want to replace the node cassandra-test-dc1-rack2-1, we first need to retrieve it's IP address from\nnodetool status for example :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"$ nodetool status\nDatacenter: dc1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address         Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  172.31.184.189  35.18 GiB  256          51.2%             9455a9bd-7a61-405e-8c3f-ee1f72f63500  rack1\nUN  172.31.180.138  37 GiB     256          51.0%             1ad1b4b7-c719-4683-8109-31aa9722c1ee  rack2\nUN  172.31.179.248  37.86 GiB  256          47.4%             69cbf178-2477-4420-ac71-6fad10f93759  rack2\nUN  172.31.182.120  41.76 GiB  256          50.2%             a4ffac86-990d-4487-80a0-b2e177d8e06e  rack1\nDN  172.31.183.213  31.14 GiB  256          51.9%             e45107ba-fe7b-4904-98cf-1373d1946bb5  rack2\nUN  172.31.181.193  33.15 GiB  256          48.4%             35806f73-17fb-4d91-b2e7-8333f393189b  rack1\n")),(0,s.kt)("p",null,"Then we can edit the ConfigMap to edit the pre_run.sh script :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cassandra-configmap-pre-run\ndata:\n  pre_run.sh: |-\n    echo "** this is a pre-scrip for run.sh that can be edit with configmap"\n    test "$(hostname)" == \'cassandra-demo-dc1-rack3-0\' && echo "-Dcassandra.replace_address_first_boot=172.31.183.213" > /etc/cassandra/jvm.options\n    echo "** end of pre_run.sh script, continue with run.sh"\n')),(0,s.kt)("p",null,"So the Operation will be :"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"Edit the configmap with the appropriate CASSANDRA_REPLACE_NODE IP for the targeted pod name"),(0,s.kt)("li",{parentName:"ol"},"delete the pvc data-cassandra-test-dc1-rack2-1"),(0,s.kt)("li",{parentName:"ol"},"the Pod will boot, execute the pre_run.sh script prior to the /run.sh"),(0,s.kt)("li",{parentName:"ol"},"the new pod replace the dead one by re-syncing the content which could take some times depending on the data size."),(0,s.kt)("li",{parentName:"ol"},"Do not forget to edit again the ConfigMap and to remove the specific line with replace_node instructions.")))}u.isMDXComponent=!0}}]);